import sys
import subprocess
from pathlib import Path
import pandas as pd
import logging
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_production_pipeline():
    """Ex√©cute le script pour traiter le fichier de production."""
    try:
        logger.info("Lancement du script de traitement du fichier de production...")
        subprocess.run([sys.executable, "-m", "Loading.insert_prod"], check=True)
    except subprocess.CalledProcessError as e:
        logger.error(f"Erreur lors de l'ex√©cution du script de production: {e}")

def run_extract_db():
    """Ex√©cute le script extract_db.py pour extraire les donn√©es de la base."""
    try:
        logger.info("D√©but de l'extraction depuis la base de donn√©es...")

        # Import direct pour r√©cup√©rer le DataFrame
        from Extract.extract_db import extract_data_to_dataframe

        df = extract_data_to_dataframe()

        if df.empty:
            logger.warning("Aucune donn√©e extraite de la base de donn√©es")
            return None

        logger.info(f"‚úÖ Extraction DB termin√©e. {len(df)} lignes extraites.")
        return df

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'ex√©cution de extract_db.py: {e}")
        return None

def run_cleaning(df: pd.DataFrame) -> pd.DataFrame:
    """Ex√©cute le nettoyage des donn√©es."""
    try:
        logger.info("D√©but du nettoyage des donn√©es...")

        from Transform.data_cleaning import clean_data_from_db

        df_clean = clean_data_from_db()

        if df_clean.empty:
            logger.warning("Aucune donn√©e apr√®s nettoyage")
            return df  # Retourne les donn√©es originales si √©chec

        logger.info(f"‚úÖ Nettoyage termin√©. {len(df_clean)} lignes trait√©es.")
        return df_clean

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du nettoyage: {e}")
        return df  # Retourne les donn√©es originales en cas d'erreur

def run_normalization(df: pd.DataFrame) -> pd.DataFrame:
    """Ex√©cute la normalisation des dates."""
    try:
        logger.info("D√©but de la normalisation des dates...")

        from Transform.date_normalization import normalize_dates_from_db

        df_normalized = normalize_dates_from_db()

        if df_normalized.empty:
            logger.warning("Aucune donn√©e apr√®s normalisation")
            return df

        logger.info(f"‚úÖ Normalisation des dates termin√©e.")
        return df_normalized

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la normalisation: {e}")
        return df

def run_unit_conversion(df: pd.DataFrame) -> pd.DataFrame:
    """Ex√©cute la conversion des unit√©s."""
    try:
        logger.info("D√©but de la conversion des unit√©s...")

        from Transform.unit_conversion import convert_units_from_db

        df_converted = convert_units_from_db()

        if df_converted.empty:
            logger.warning("Aucune donn√©e apr√®s conversion")
            return df

        logger.info(f"‚úÖ Conversion des unit√©s termin√©e.")
        return df_converted

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la conversion: {e}")
        return df

def run_transform_pipeline(df: pd.DataFrame) -> pd.DataFrame:
    """Ex√©cute toute la logique de transformation dans l'ordre correct."""
    logger.info("üöÄ D√©marrage du pipeline de transformation...")

    # Ordre correct du pipeline :
    # 1. Cleaning des donn√©es
    df = run_cleaning(df)

    # 2. Normalisation des dates
    df = run_normalization(df)

    # 3. Conversion des unit√©s
    df = run_unit_conversion(df)

    logger.info("‚ú® Pipeline Transform termin√© avec succ√®s!")
    return df

def display_pipeline_summary(df: pd.DataFrame):
    """Affiche un r√©sum√© du pipeline."""
    print("\n" + "="*50)
    print("üìä R√âSUM√â DU PIPELINE")
    print("="*50)
    print(f"üìà Nombre total d'enregistrements: {len(df)}")
    print(f"üè∑Ô∏è  Nombre de colonnes: {len(df.columns)}")
    print(f"üìã Colonnes disponibles: {list(df.columns)}")

    # Afficher les statistiques de base
    if not df.empty:
        print(f"\nüìä Aper√ßu des donn√©es:")
        print(df.head())

        # Informations sur les valeurs manquantes
        missing_values = df.isnull().sum()
        if missing_values.sum() > 0:
            print(f"\n‚ö†Ô∏è  Valeurs manquantes par colonne:")
            for col, count in missing_values[missing_values > 0].items():
                print(f"   - {col}: {count} valeurs manquantes")
        else:
            print(f"\n‚úÖ Aucune valeur manquante d√©tect√©e")

def main():
    """Ex√©cute le pipeline complet selon la logique d√©finie."""
    logger.info("üéØ D√©marrage du pipeline complet...")

    # √âtape 0: V√©rification du fichier de production
    from Loading.insert_prod import get_current_production_file

    try:
        file_path = get_current_production_file()
        logger.info(f"Fichier de production trouv√© : {file_path}")
        logger.info("Lancement du traitement du fichier de production...")
        run_production_pipeline()
    except FileNotFoundError:
        logger.info("Aucun fichier de production d√©tect√©. Lancement de l'extraction depuis la base de donn√©es...")

        # √âtape 1: Extraction depuis la base de donn√©es
        df = run_extract_db()

        if df is None or df.empty:
            logger.error("‚ùå √âchec de l'extraction des donn√©es. Arr√™t du pipeline.")
            return

        print(f"\nüì• Donn√©es initiales extraites: {len(df)} lignes, {len(df.columns)} colonnes")

        # √âtape 2: Transformation des donn√©es
        df = run_transform_pipeline(df)

        # √âtape 3: Affichage du r√©sum√©
        display_pipeline_summary(df)

    logger.info("üéâ Pipeline termin√© avec succ√®s!")

if __name__ == "__main__":
    main()
